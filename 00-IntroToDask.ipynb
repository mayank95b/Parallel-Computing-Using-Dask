{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Dask is a parallel computing library that scales the existing Python ecosystem.it is popularly known as a ‘parallel computing’ python library that has been designed to run across multiple systems. Your next question would understandably be – what is parallel computing?\n",
    "\n",
    "parallel computation is performing multiple tasks (or computations) simultaneously, using more than one resource.\n",
    "\n",
    "- Dask can efficiently perform parallel computations on a single machine using multi-core CPUs. For example, if you have a quad core processor, Dask can effectively use all 4 cores of your system simultaneously for processing. \n",
    "- In order to use lesser memory during computations, Dask stores the complete data on the disk, and uses chunks of data (smaller parts, rather than the whole data) from the disk for processing. During the processing, the intermediate values generated (if any) are discarded as soon as possible, to save the memory consumption.\n",
    "\n",
    "## Features :\n",
    "- Acts as an Drop and replacement.\n",
    "- It can efficiently perform parallel computations on a single machine using multi-core CPUs.\n",
    "- it stores the complete data on the disk, and uses chunks of data (smaller parts, rather than the whole data) from the disk   for processing.\n",
    "\n",
    "<img src=\"images/fail-case.gif\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask supports the Pandas dataframe and Numpy array data structures to analyze large datasets. Basically, Dask lets you scale pandas and numpy with minimum changes in your code format. How great is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask emphasizes the following virtues:\n",
    "\n",
    "- Familiar: Provides parallelized NumPy array and Pandas DataFrame objects\n",
    "- Flexible: Provides a task scheduling interface for more custom workloads and integration with other projects.\n",
    "- Native: Enables distributed computing in pure Python with access to the PyData stack.\n",
    "- Fast: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\n",
    "- Scales up: Runs resiliently on clusters with 1000s of cores\n",
    "- Scales down: Trivial to set up and run on a laptop in a single process\n",
    "- Responsive: Designed with interactive computing in mind, it provides rapid feedback and diagnostics to aid humans\n",
    "\n",
    "<img src=\"images/taskgraph.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface\n",
    "Dask supports several user interfaces:\n",
    "#### High-Level\n",
    "- Arrays: Parallel NumPy\n",
    "- Bags: Parallel lists\n",
    "- DataFrames: Parallel Pandas\n",
    "- Machine Learning : Parallel Scikit-Learn\n",
    "\n",
    "#### Low-Level\n",
    "- Delayed: Parallel function evaluation\n",
    "- Futures: Real-time parallel function evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dask\n",
    "You can install dask with **conda**, with **pip**, or by installing from source.\n",
    "\n",
    "#### Using conda\n",
    "Dask is installed in Anaconda by default. You can update it using the following command:\n",
    "\n",
    "- `conda install dask`\n",
    "\n",
    "#### Using Pip\n",
    "\n",
    "To install Dask using pip, simply use the below code in your command prompt/terminal window:\n",
    "\n",
    "- `pip install “dask[complete]\"`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling\n",
    "\n",
    "All of the large-scale Dask collections like **Dask Array**, **Dask DataFrame**, and **Dask Bag** and the fine-grained APIs like **delayed** and **futures** generate task graphs where each node in the graph is a normal Python function and edges between nodes are normal Python objects that are created by one task as outputs and used as inputs in another task. After Dask generates these task graphs, it needs to execute them on parallel hardware. This is the job of a task scheduler. Different task schedulers exist, and each will consume a task graph and compute the same result, but with different performance characteristics.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/schedular.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "Now, let's officially start.\n",
    "\n",
    "- [Dask Array](01-DaskArray.ipynb)\n",
    "- [Dask Bags](02-bag.ipynb)\n",
    "- [Dask DataFrames](03-Dataframe.ipynb)\n",
    "- [Dask ML](04-DaskML.ipynb)\n",
    "- [Dask.Delayed](05-delayed.ipynb)\n",
    "- [Dask.futures](06-futures.ipynb)\n",
    "- [Dask Daignostic](07-Diagnostic.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
